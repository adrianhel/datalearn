## 7.7.1 Действия (Actions) в Apache Spark

### [Назад в Модуль 7 ⤶](/data/Module7/readme.md)

- Действия инициируют выполнение вычислений, описанных преобразованиями, и возвращают результат на драйвер или записывают 
его во внешний источник.  
- Действия заставляют Spark выполнить все отложенные преобразования, необходимые для получения результата.  

## 7.7.2 Типы действий
- **collect()**: Возвращает все элементы RDD или DataFrame в виде списка на драйвер.  
- **count()**: Возвращает количество элементов в RDD или DataFrame.  
- **first()**: Возвращает первый элемент RDD или DataFrame.  
- **take(n)**: Возвращает первые n элементов RDD или DataFrame.  
- **reduce(func)**: Агрегирует элементы с помощью функции. 
- **foreach(func)**: Применяет функцию к каждому элементу (например, для сохранения в базу данных).
- **saveAsTextFile(path)**: Сохраняет RDD как текстовый файл в файловой системе.  

```python
# Применение действия collect для выполнения вычислений и получения результата
result = squared_rdd.collect()
print(result)  # [1, 4, 9, 16, 25]
```

## 7.7.3 Что происходит при вызове действия?
Spark:
1. Смотрит на построенный DAG.  
2. Оптимизирует его (например, объединяя map и filter в одну стадию).  
3. Разбивает план на стадии (stages) и задачи (tasks).  
4. Распределяет задачи по исполнителям (executors) в кластере.  
5. Выполняет вычисления и возвращает результат.  

## 7.7.4 Основные действия с RDD
Ниже представлены основные действия, часто используемые при работе с RDD:  

### collect()
Собирает все элементы RDD на драйверскую машину в виде массива.  
Применяется, когда объем данных невелик и может быть размещен в памяти драйвера.  

```scala
val data = sc.parallelize(Seq(1, 2, 3, 4, 5))
val result = data.collect()
// result: Array(1, 2, 3, 4, 5)
```
                  
### count()
Возвращает количество элементов в RDD как число типа Long.  
Используется для быстрой оценки размера набора данных.  

```scala
val rdd = sc.parallelize(Seq("a", "b", "c"))
val n = rdd.count()
// n: Long = 3
```
                  
### take(n: Int)
Возвращает массив из первых n элементов RDD.  
Используется для предварительного просмотра данных.  

```scala
val rdd = sc.parallelize(1 to 100)
val top5 = rdd.take(5)
// top5: Array(1, 2, 3, 4, 5)
```
                  
### reduce(func: (T, T) ⇒ T)
Агрегирует элементы RDD с помощью указанной коммутативной и ассоциативной бинарной функции.  
Результатом является одно значение типа T.  

```scala
val rdd = sc.parallelize(Seq(1, 2, 3, 4))
val sum = rdd.reduce((x, y) => x + y)
// sum: Int = 10
```
                  
### fold(zeroValue: T)(func: (T, T) ⇒ T)
Агрегирует элементы RDD с помощью начального значения zeroValue и бинарной функции.  
Гарантирует, что zeroValue будет применено к каждому разделу (partition) как стартовое значение.  

```scala
val rdd = sc.parallelize(Seq(1, 2, 3))
val res = rdd.fold(0)((x, y) => x + y)
// res: Int = 6
```
                  
### aggregate(zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)
Позволяет проводить агрегирование с разными типами для промежуточных и финальных результатов.  
Используется для вычисления агрегатов с сохранением структуры, например, вычисление максимума и суммы одновременно.  

```scala
val rdd = sc.parallelize(Seq(1, 2, 3, 4))
val res = rdd.aggregate((0, 0))(
  (acc, v) => (acc._1 + v, math.max(acc._2, v)),
  (acc1, acc2) => (acc1._1 + acc2._1, math.max(acc1._2, acc2._2))
)
// res: (Int, Int) = (10, 4)
```
                  
### foreach(func: T ⇒ Unit)
Применяет функцию к каждому элементу RDD без возврата значения.  
Часто используется для вывода или побочных эффектов.  

```scala
val rdd = sc.parallelize(Seq("apple", "banana"))
rdd.foreach(println)
```
                  
### saveAsTextFile(path: String)
Сохраняет элементы RDD в текстовый файл или директорию в файловой системе (например, HDFS, локальная файловая система).  

```scala
val rdd = sc.parallelize(Seq("foo", "bar"))
rdd.saveAsTextFile("output/path")
```
                  
### saveAsSequenceFile(path: String)
Сохраняет пары ключ-значение в формате SequenceFile (только для парных RDD).  

```scala
val rdd = sc.parallelize(Seq(("a", 1), ("b", 2)))
rdd.saveAsSequenceFile("output/seq")
```
                  
### countByValue()
Возвращает Map, содержащий количество вхождений каждого уникального элемента.  
Часто используется для анализа распределения значений.  

```scala
val rdd = sc.parallelize(Seq("cat", "dog", "cat"))
val counts = rdd.countByValue()
// counts: Map(cat -> 2, dog -> 1)
```
                  
## 7.7.5 Принципы работы действий
- Действия инициируют физическое выполнение DAG всех трансформаций, накопленных на RDD.  
- Результатом действия может быть объект в драйверской программе (например, Array или Map) или сохранение данных во 
внешнем хранилище (файл, база данных).  
- Каждое действие потенциально повторно вычисляет все зависимости, если промежуточные данные не были закэшированы 
или сохранены (persisted).  
- Для минимизации вычислений часто используют методы кэширования (cache, persist).  

## 7.7.6 Примеры практического применения
- **Анализ журналов событий (логов):**
    - Подсчет количества ошибок с помощью `filter` и `count()`.  

```scala
val logs = sc.textFile("logs.txt")
val errorCount = logs.filter(line => line.contains("ERROR")).count()
```
                  
- **Агрегирование данных:**
    - Подсчет общего объема продаж с помощью `reduce` или `aggregate`.  

```scala
val sales = sc.parallelize(Seq(120, 250, 80))
val total = sales.reduce(_ + _)
// total: Int = 450
```
                  
- **Сохранение результатов анализа:**
    - Экспорт уникальных пользователей в файл с помощью `distinct` и `saveAsTextFile`.  

```scala
val users = sc.textFile("access.log").map(_.split(" ")(0)).distinct()
users.saveAsTextFile("unique_users")
```

## 7.7.7 Подходы к оптимизации вычислений
- Использование `cache()` и `persist()` для хранения промежуточных результатов между действиями.  
- Сведение количества действий к минимуму за счет объединения логики в одно действие, если возможно.  
- Использование действий, возвращающих агрегированные данные (например, `count()`, `reduce()`), для экономии сетевого 
трафика и ресурсов драйвера.  