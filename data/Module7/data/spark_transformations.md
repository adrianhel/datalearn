## 7.7.1 Преобразования (Transformations) в Apache Spark

### [Назад в Модуль 7 ⤶](/data/Module7/readme.md)

- Преобразования создают новый RDD (или DataFrame/Dataset) на основе существующего, но не выполняют вычислений немедленно. 
Они возвращают новый объект (RDD или DataFrame), который описывает результат трансформации.  
- Преобразования являются ленивыми и только определяют, как должны быть преобразованы данные, без фактического 
выполнения операций.  

## 7.7.2 Типы преобразований
- **map(func)**: Применяет функцию к каждому элементу RDD и возвращает новый RDD.  
- **filter(func)**: Возвращает новый RDD, содержащий только те элементы, которые удовлетворяют условию.  
- **flatMap(func)**: Похож на map, но каждая входная строка может быть сопоставлена с несколькими выходными.   
- **join()**: Соединяет RDD.  
- **union()**: Объединяет два RDD в один.  
- **distinct()**: Убирает дублирующиеся элементы из RDD.  
- **groupByKey()**, **reduceByKey()**: Используются для парных RDD (ключ-значение).

```python
from pyspark import SparkContext

sc = SparkContext("local", "Transformations Example")
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

# Применение преобразования map
squared_rdd = rdd.map(lambda x: x * x)
```

## 7.7.3 Что происходит при вызове трансформации?
Ничего, кроме обновления внутреннего DAG.

```python
lines_rdd = sc.textFile("data.txt")  # Еще ничего не прочитано
words_rdd = lines_rdd.flatMap(lambda line: line.split(" ")) # Ничего не выполнено
filtered_rdd = words_rdd.filter(lambda word: word.startswith("a")) # Все еще ничего
```

На этом этапе у Spark есть план: `Прочитать файл -> разбить на слова -> отфильтровать слова на 'a'`.