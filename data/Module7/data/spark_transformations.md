## 7.7.1 Преобразования (Transformations) в Apache Spark

### [Назад в Модуль 7 ⤶](/data/Module7/readme.md)

- Преобразования создают новый RDD (или DataFrame/Dataset) на основе существующего, но не выполняют вычислений немедленно. 
Они возвращают новый объект (RDD или DataFrame), который описывает результат трансформации.  
- Преобразования являются ленивыми и только определяют, как должны быть преобразованы данные, без фактического 
выполнения операций.  

## 7.7.2 Классификация трансформаций
- **Нефильтрующие (map-подобные)** — преобразуют каждое отдельное значение или группу значений без 
изменения структуры разбиения:  
    - **map(func)**: Применяет функцию к каждому элементу RDD и возвращает новый RDD.  
    - **flatMap(func)**: Похож на map, но каждая входная строка может быть сопоставлена с несколькими выходными.  
    - **mapPartitions**   
    - **mapPartitionsWithIndex**  

- **Фильтрующие** — отбирают часть данных на основе заданного условия:  
    - **filter(func)**: Возвращает новый RDD, содержащий только те элементы, которые удовлетворяют условию.  
    - **distinct()**: Убирает дублирующиеся элементы из RDD.  
    - **sample**  

- **Агрегирующие и группирующие** — агрегируют, объединяют или группируют данные по ключу:  
    - **groupByKey()**: Используется для парных RDD (ключ-значение).  
    - **reduceByKey**: Используется для парных RDD (ключ-значение).  
    - **aggregateByKey**  
    - **combineByKey**  
    - **groupBy**  

- **Сортирующие** — изменяют порядок элементов:  
    - **sortBy**  
    - **sortByKey**  

- **Объединяющие и соединяющие** — работают с несколькими RDD:  
    - **union()**: Объединяет пару RDD в один.  
    - **intersection**  
    - **subtract**  
    - **cartesian**  
    - **join()**: Соединяет пару RDD.  
    - **leftOuterJoin**  
    - **rightOuterJoin**  
    - **fullOuterJoin**  
    - **cogroup**  

## 7.7.3 Что происходит при вызове трансформации?
Ничего, кроме обновления внутреннего DAG.

```python
lines_rdd = sc.textFile("data.txt")  # Еще ничего не прочитано
words_rdd = lines_rdd.flatMap(lambda line: line.split(" ")) # Ничего не выполнено
filtered_rdd = words_rdd.filter(lambda word: word.startswith("a")) # Все еще ничего
```

На этом этапе у Spark есть план: `Прочитать файл -> разбить на слова -> отфильтровать слова на 'a'`.  