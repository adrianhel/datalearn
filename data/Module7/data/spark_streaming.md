## 7.10.1 Spark Streaming: потоковая обработка

### [Назад в Модуль 7 ⤶](/data/Module7/readme.md)

**Поток данных (Data Stream)** — непрерывная последовательность событий или сообщений, поступающих от источников 
данных в реальном времени, таких как сенсоры, логи серверов, сообщения в социальных сетях.  
  
**Мини-батч (Micro-batch)** — основной принцип работы Spark Streaming; поток данных разбивается на небольшие 
интервалы времени, в течение которых собранные данные обрабатываются как отдельный пакет (batch).  
  
**DStream (Discretized Stream)** — основной абстракцией потоковой обработки в Spark Streaming является 
дискретизированный поток. DStream представляет собой последовательность RDD, каждый из которых соответствует данным, 
собранным за определённый интервал времени.  
  
**RDD (Resilient Distributed Dataset)** — устойчивая распределённая коллекция данных, базовый элемент вычислений в Spark, 
обладающий свойствами отказоустойчивости и параллельности.  
  
**Источник данных (Input Source)** — внешний поток, из которого Spark Streaming получает данные. Примеры: Apache Kafka, 
Apache Flume, TCP-сокеты, HDFS, файловые системы.  
  
**Выходной поток (Output Sink)** — место назначения обработанных данных, например, файловая система, база данных, 
внешний сервис.  

## 7.10.2 Архитектура Spark Streaming
- **Driver** — основной управляющий процесс, координирующий выполнение задач, создание DStream, распределение работы 
между рабочими узлами.  
- **Executor** — рабочие процессы, исполняющие задачи обработки данных, хранящие части RDD и выполняющие трансформации 
и действия над ними.  
- **Receiver** — специальный компонент, принимающий данные из внешнего источника и инкапсулирующий их в RDD для 
последующей обработки.  

## 7.10.3 Принципы работы Spark Streaming
1. **Получение данных:** Spark Streaming подключается к одному или нескольким потоковым источникам и получает входящие
данные.  
2. **Разбиение на мини-батчи:** Входящий поток разбивается на интервалы времени (обычно от сотых долей секунды 
до нескольких секунд), каждый из которых формирует отдельный RDD.  
3. **Обработка данных:** К каждому RDD применяются операции трансформации и действия, аналогично пакетной 
обработке в Spark.  
4. **Вывод результата:** Обработанные данные могут быть сохранены в различных внешних системах или переданы далее по 
потоку.  

## 7.10.4 Основные операции над DStream
- **Трансформации (Transformations)** — операции, создающие новый DStream из существующего, например `map`, `flatMap`, 
`filter`, `reduceByKey`, `window`.  
- **Действия (Actions)** — операции, отправляющие данные из DStream во внешний вывод, такие как `saveAsTextFiles`, 
`foreachRDD`.  
- **Оконные операции (Window Operations)** — позволяют анализировать данные за скользящий временной интервал.  
Основные параметры:  
    - **window length** — длина окна (например, 10 секунд);  
    - **slide interval** — шаг сдвига окна (например, 5 секунд).  

