## 7.10.1 Spark Streaming: потоковая обработка

### [Назад в Модуль 7 ⤶](/data/Module7/readme.md)

**Поток данных (Data Stream)** — непрерывная последовательность событий или сообщений, поступающих от источников 
данных в реальном времени, таких как сенсоры, логи серверов, сообщения в социальных сетях.  
  
**Мини-батч (Micro-batch)** — основной принцип работы Spark Streaming; поток данных разбивается на небольшие 
интервалы времени, в течение которых собранные данные обрабатываются как отдельный пакет (batch).  
  
**DStream (Discretized Stream)** — основной абстракцией потоковой обработки в Spark Streaming является 
дискретизированный поток. DStream представляет собой последовательность RDD, каждый из которых соответствует данным, 
собранным за определённый интервал времени.  
  
**RDD (Resilient Distributed Dataset)** — устойчивая распределённая коллекция данных, базовый элемент вычислений в Spark, 
обладающий свойствами отказоустойчивости и параллельности.  
  
**Источник данных (Input Source)** — внешний поток, из которого Spark Streaming получает данные. Примеры: Apache Kafka, 
Apache Flume, TCP-сокеты, HDFS, файловые системы.  
  
**Выходной поток (Output Sink)** — место назначения обработанных данных, например, файловая система, база данных, 
внешний сервис.  

## 7.10.2 Архитектура Spark Streaming
- **Driver** — основной управляющий процесс, координирующий выполнение задач, создание DStream, распределение работы 
между рабочими узлами.  
- **Executor** — рабочие процессы, исполняющие задачи обработки данных, хранящие части RDD и выполняющие трансформации 
и действия над ними.  
- **Receiver** — специальный компонент, принимающий данные из внешнего источника и инкапсулирующий их в RDD для 
последующей обработки.  

## 7.10.3 Принципы работы Spark Streaming
1. **Получение данных:** Spark Streaming подключается к одному или нескольким потоковым источникам и получает входящие
данные.  
2. **Разбиение на мини-батчи:** Входящий поток разбивается на интервалы времени (обычно от сотых долей секунды 
до нескольких секунд), каждый из которых формирует отдельный RDD.  
3. **Обработка данных:** К каждому RDD применяются операции трансформации и действия, аналогично пакетной 
обработке в Spark.  
4. **Вывод результата:** Обработанные данные могут быть сохранены в различных внешних системах или переданы далее по 
потоку.  

## 7.10.4 Основные операции над DStream
- **Трансформации (Transformations)** — операции, создающие новый DStream из существующего, например `map`, `flatMap`, 
`filter`, `reduceByKey`, `window`.  
- **Действия (Actions)** — операции, отправляющие данные из DStream во внешний вывод, такие как `saveAsTextFiles`, 
`foreachRDD`.  
- **Оконные операции (Window Operations)** — позволяют анализировать данные за скользящий временной интервал.  
Основные параметры:  
    - **window length** — длина окна (например, 10 секунд);  
    - **slide interval** — шаг сдвига окна (например, 5 секунд).  

## 7.10.5 Работа с внешними источниками данных
- **Apache Kafka** — популярный брокер сообщений для сбора и передачи потоковых данных. Spark Streaming поддерживает 
Kafka через интеграцию с Kafka API.  
- **Apache Flume** — система сбора и агрегирования логов, которая может быть использована как источник для 
Spark Streaming.  
- **Файловые системы** — поддержка мониторинга директорий и обработки новых файлов по мере их появления.  

### Пример интеграции с Kafka на Scala

```scala
import org.apache.spark.streaming.kafka010._

val kafkaParams = Map(
    "bootstrap.servers" -> "localhost:9092",
    "key.deserializer" -> "org.apache.kafka.common.serialization.StringDeserializer",
    "value.deserializer" -> "org.apache.kafka.common.serialization.StringDeserializer",
    "group.id" -> "spark-streaming-group"
)
val topics = Array("topic1")
val stream = KafkaUtils.createDirectStream[String, String](
    ssc,
    LocationStrategies.PreferConsistent,
    ConsumerStrategies.Subscribe[String, String](topics, kafkaParams)
)

val lines = stream.map(record => record.value)
```

## 7.10.6 Гарантии доставки и отказоустойчивость
- **Exactly-once** семантика — Spark Streaming способен гарантировать, что каждое событие будет обработано 
ровно один раз при корректной настройке источников и выводов.  
- **Checkpointing** — механизм сохранения состояния потоковой обработки на диск для восстановления после сбоев. 
Может хранить состояние DStream, метаданные и данные о прогрессе.  
- **Операции с состоянием (Stateful transformations)**, такие как `updateStateByKey` и `mapWithState`, требуют 
использования checkpointing.  

### Пример использования checkpointing

```scala
ssc.checkpoint("hdfs://path/to/checkpoint")

val stateDstream = pairs.updateStateByKey(
    (newValues: Seq[Int], runningCount: Option[Int]) =>
        Some(runningCount.getOrElse(0) + newValues.sum)
)
```

## 7.10.7 Сравнение с другими системами потоковой обработки
**Micro-batch processing** — Spark Streaming обрабатывает данные пакетами с малым интервалом (например, 1 секунда), 
что обеспечивает низкую задержку и высокую производительность, но не является обработкой событий поштучно.  

**Continuous Processing** — альтернативные системы, такие как Apache Flink и Apache Storm, реализуют истинную обработку 
событий в режиме реального времени (event-at-a-time), что может обеспечить ещё меньшие задержки.  

## 7.10.8 Практические применения Spark Streaming
- Анализ и мониторинг логов серверов и приложений в реальном времени  
- Обнаружение мошенничества (fraud detection) в финансовых транзакциях  
- Аналитика событий в социальных сетях и системах рекомендаций  
- Мониторинг IoT устройств и сенсоров  
- Системы оповещения и реагирования в реальном времени  

## 7.10.9 Лимитации и эволюция Spark Streaming
- **Задержка микробатча** — минимальная задержка зависит от интервала батча; для минимальных задержек рекомендуется 
использовать Structured Streaming.  
- **Structured Streaming** — новое поколение потоковой обработки в Spark, основанное на Spark SQL и DataFrame API, 
с поддержкой непрерывной обработки и более гибкой семантикой триггеров.  
- **Stateful processing** — Spark Streaming поддерживает хранение и обновление состояния между пакетами данных, 
что важно для сложных сценариев обработки.  

