## 7.4.1 Spark Session и Spark Context

### [Назад в Модуль 7 ⤶](/data/Module7/readme.md)

В **Apache Spark** создание **Spark Session** является более предпочтительным подходом по сравнению с использованием 
**Spark Context**, т.к. Spark Context был основной входной точкой для программирования на Spark в более ранних версиях 
фреймворка. Он предоставлял основные функции и возможности Spark, такие, как создание **RDD** (Resilient Distributed Dataset) 
и выполнение операций над ними. Однако, с появлением **Spark 2.0** и выше, введение Spark Session стало рекомендованным 
способом работы со Spark.  

## 7.4.2 Преимущества Spark Session по сравнению с Spark Context
1. **Удобство использования**: Spark Session предоставляет более удобный и единый интерфейс для работы с различными 
модулями Spark. Он упрощает кодирование и повышает производительность разработки.
2. **Поддержка различных типов данных**: Spark Session поддерживает работу с **RDD**, **DataFrames** и **Datasets**, что 
обеспечивает более гибкую обработку и анализ данных в Spark.
3. **Интеграция со сторонними инструментами**: Spark Session обеспечивает интеграцию с различными инструментами и 
библиотеками, такими как **Hive**, **JDBC**, **Parquet**, **Avro** и другими.
4. **Улучшенная оптимизация**: Spark Session имеет лучшую оптимизацию выполнения запросов, что приводит к улучшенной 
производительности.
5. **Поддержка различных источников данных**: Spark Session позволяет работать с различными источниками данных, 
включая файловые системы (**HDFS**, **S3** и т.д.), базы данных (**MySQL**, **PostgreSQL** и др.), **Apache Kafka** и другие.  

## 7.4.3 Примеры создания
#### Пример создания Spark Context на Python

```python
from pyspark import SparkContext

# Создание объекта SparkContext
sc = SparkContext(appName="MySparkApp")

# Здесь можно выполнять операции с RDD

# Закрытие SparkContext
sc.stop()
```
        
#### Пример создания Spark Session на Python

```python
from pyspark.sql import SparkSession

# Создание объекта SparkSession
spark = SparkSession.builder \
    .appName("MySparkApp") \
    .getOrCreate()

# Здесь можно выполнять операции с DataFrames
# Закрытие SparkSession
spark.stop()
```
                  
В обоих примерах мы используем PySpark для работы со Spark на языке Python.  

В примере создания Spark Context мы импортируем модуль SparkContext из библиотеки PySpark и создаем объект `sc` с 
указанием имени приложения. Затем мы можем выполнять операции с RDD, используя sc. По завершении работы необходимо 
вызвать метод `stop()` для закрытия Spark Context.  

В примере создания Spark Session мы импортируем модуль SparkSession и используем `builder` для настройки параметров 
Spark Session, таких, как имя приложения. Затем мы вызываем метод `getOrCreate()`, который создает новую Spark Session 
или возвращает существующую, если она уже создана. После этого мы можем выполнять операции с DataFrames, используя Spark. 
Наконец, мы вызываем метод `stop()` для закрытия Spark Session.  