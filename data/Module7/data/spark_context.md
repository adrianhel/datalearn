## 7.4 Spark Session и Spark Context

### [Назад в Модуль 7 ⤶](/data/Module7/readme.md)

В **Apache Spark** создание **Spark Session** является более предпочтительным подходом по сравнению с использованием 
**Spark Context**, т.к. Spark Context был основной входной точкой для программирования на Spark в более ранних версиях 
фреймворка. Он предоставлял основные функции и возможности Spark, такие, как создание **RDD** (Resilient Distributed Dataset) 
и выполнение операций над ними. Однако, с появлением **Spark 2.0** и выше, введение Spark Session стало рекомендованным 
способом работы с Spark.  

