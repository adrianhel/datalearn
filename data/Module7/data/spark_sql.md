## 7.9.1 Spark SQL: обработка структурированных данных

### [Назад в Модуль 7 ⤶](/data/Module7/readme.md)

**Структурированные данные** — данные, обладающие чёткой схемой (schema), определяющей типы и имена столбцов 
(таблицы баз данных, CSV-файлы, Parquet-файлы).  
**Schema** — описание структуры данных, включающее типы и имена столбцов.  
**DataFrame** — распределённая коллекция данных, организованных в виде именованных столбцов, аналогичная таблице 
в реляционной базе данных. DataFrame реализует «ленивые» вычисления и поддерживает множество оптимизаций.  
**Dataset** — типизированная абстракция, предоставляющая статическую типизацию и мощные средства трансформации 
данных (доступна в Scala и Java).  
**SQLContext** / **SparkSession** — точка входа для работы с Spark SQL. С версии Spark 2.0 основной 
интерфейс — `SparkSession`.  

## 7.9.2 Архитектура и принципы работы Spark SQL
Spark SQL поддерживает два основных способа работы с данными:  
1. Выполнение запросов на языке SQL  
2. Манипулирование данными с помощью API DataFrame/Dataset  

Ядро Spark SQL реализует оптимизатор запросов **Catalyst**, который автоматически анализирует, оптимизирует и преобразует 
планы выполнения запросов для достижения максимальной производительности. Для хранения и передачи данных используется 
оптимизированный формат **Tungsten**.  

### Поток выполнения запроса
1. Разбор (Parsing): SQL-запрос преобразуется в логический план.  
2. Анализ (Analysis): проверка схемы, синтаксиса и разрешение имён столбцов.  
3. Оптимизация (Optimization): применение правил **Catalyst** для преобразования логического плана в более эффективный.  
4. Планирование (Planning): генерация физического плана выполнения.  
5. Выполнение (Execution): выполнение физического плана на кластере Spark.  

## 7.9.3 Работа с DataFrame
DataFrame — основной объект Spark SQL для представления структурированных данных. Его можно создать из различных 
источников данных: файлов (JSON, Parquet, ORC, CSV), баз данных, RDD и других.  

#### Пример создания SparkSession

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("SparkSQLExample") \
    .getOrCreate()
```
                  
#### Загрузка данных в DataFrame

```python
# Загрузка данных из CSV-файла
df = spark.read.csv("path/to/data.csv", header=True, inferSchema=True)

# Загрузка данных из Parquet-файла
df_parquet = spark.read.parquet("path/to/data.parquet")
```
                  
#### Работа с DataFrame: основные операции
- **Выборка столбцов:**

    ```python
    df.select("name", "age")
    ```
                  
- **Фильтрация строк:**

    ```python
    df.filter(df.age > 21)
    ```
                  
- **Группировка и агрегирование:**

    ```python
    df.groupBy("city").agg({"salary": "avg"})
    ```
                  
- **Соединения (joins):**

    ```python
    df1.join(df2, df1.id == df2.id, "inner")
    ```
                  
- **Сортировка:**

    ```python
    df.orderBy(df.age.desc())
    ```
                  
