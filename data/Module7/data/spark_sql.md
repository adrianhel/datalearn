## 7.9.1 Spark SQL: обработка структурированных данных

### [Назад в Модуль 7 ⤶](/data/Module7/readme.md)

**Структурированные данные** — данные, обладающие чёткой схемой (schema), определяющей типы и имена столбцов 
(таблицы баз данных, CSV-файлы, Parquet-файлы).  
**Schema** — описание структуры данных, включающее типы и имена столбцов.  
**DataFrame** — распределённая коллекция данных, организованных в виде именованных столбцов, аналогичная таблице 
в реляционной базе данных. DataFrame реализует «ленивые» вычисления и поддерживает множество оптимизаций.  
**Dataset** — типизированная абстракция, предоставляющая статическую типизацию и мощные средства трансформации 
данных (доступна в Scala и Java).  
**SQLContext** / **SparkSession** — точка входа для работы с Spark SQL. С версии Spark 2.0 основной 
интерфейс — `SparkSession`.  

## 7.9.2 Архитектура и принципы работы Spark SQL
Spark SQL поддерживает два основных способа работы с данными:  
1. Выполнение запросов на языке SQL  
2. Манипулирование данными с помощью API DataFrame/Dataset  

Ядро Spark SQL реализует оптимизатор запросов Catalyst, который автоматически анализирует, оптимизирует и преобразует 
планы выполнения запросов для достижения максимальной производительности. Для хранения и передачи данных используется 
оптимизированный формат **Tungsten**.  

### Поток выполнения запроса
1. Разбор (Parsing): SQL-запрос преобразуется в логический план.  
2. Анализ (Analysis): проверка схемы, синтаксиса и разрешение имён столбцов.  
3. Оптимизация (Optimization): применение правил Catalyst для преобразования логического плана в более эффективный.  
4. Планирование (Planning): генерация физического плана выполнения.  
5. Выполнение (Execution): выполнение физического плана на кластере Spark.  

