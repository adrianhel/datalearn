## 7.9.1 Spark SQL: обработка структурированных данных

### [Назад в Модуль 7 ⤶](/data/Module7/readme.md)

**Структурированные данные** — данные, обладающие чёткой схемой (schema), определяющей типы и имена столбцов 
(таблицы баз данных, CSV-файлы, Parquet-файлы).  
**Schema** — описание структуры данных, включающее типы и имена столбцов.  
**DataFrame** — распределённая коллекция данных, организованных в виде именованных столбцов, аналогичная таблице 
в реляционной базе данных. DataFrame реализует «ленивые» вычисления и поддерживает множество оптимизаций.  
**Dataset** — типизированная абстракция, предоставляющая статическую типизацию и мощные средства трансформации 
данных (доступна в Scala и Java).  
**SQLContext** / **SparkSession** — точка входа для работы с Spark SQL. С версии Spark 2.0 основной 
интерфейс — `SparkSession`.  

## 7.9.2 Архитектура и принципы работы Spark SQL
Spark SQL поддерживает два основных способа работы с данными:  
1. Выполнение запросов на языке SQL  
2. Манипулирование данными с помощью API DataFrame/Dataset  

Ядро Spark SQL реализует оптимизатор запросов **Catalyst**, который автоматически анализирует, оптимизирует и преобразует 
планы выполнения запросов для достижения максимальной производительности. Для хранения и передачи данных используется 
оптимизированный формат **Tungsten**.  

### Поток выполнения запроса
1. Разбор (Parsing): SQL-запрос преобразуется в логический план.  
2. Анализ (Analysis): проверка схемы, синтаксиса и разрешение имён столбцов.  
3. Оптимизация (Optimization): применение правил **Catalyst** для преобразования логического плана в более эффективный.  
4. Планирование (Planning): генерация физического плана выполнения.  
5. Выполнение (Execution): выполнение физического плана на кластере Spark.  

## 7.9.3 Работа с DataFrame
DataFrame — основной объект Spark SQL для представления структурированных данных. Его можно создать из различных 
источников данных: файлов (JSON, Parquet, ORC, CSV), баз данных, RDD и других.  

#### Пример создания SparkSession

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("SparkSQLExample") \
    .getOrCreate()
```
                  
#### Загрузка данных в DataFrame

```python
# Загрузка данных из CSV-файла
df = spark.read.csv("path/to/data.csv", header=True, inferSchema=True)

# Загрузка данных из Parquet-файла
df_parquet = spark.read.parquet("path/to/data.parquet")
```
                  
### Основные операции
- **Выборка столбцов:**

    ```python
    df.select("name", "age")
    ```
                  
- **Фильтрация строк:**

    ```python
    df.filter(df.age > 21)
    ```
                  
- **Группировка и агрегирование:**

    ```python
    df.groupBy("city").agg({"salary": "avg"})
    ```
                  
- **Соединения (joins):**

    ```python
    df1.join(df2, df1.id == df2.id, "inner")
    ```
                  
- **Сортировка:**

    ```python
    df.orderBy(df.age.desc())
    ```

## 7.9.4 Работа с SQL-запросами
Spark SQL позволяет выполнять SQL-запросы к DataFrame, регистрируя их как временные (**temporary**) 
или постоянные (**global temporary**) представления (**views**).  

```python
# Регистрация DataFrame как временного представления
df.createOrReplaceTempView("people")

# Выполнение SQL-запроса
result = spark.sql("SELECT name, COUNT(*) FROM people GROUP BY name")
```
                  
В глобальном контексте временные представления доступны только в рамках одной сессии, глобальные временные — 
во всех сессиях Spark.  

## 7.9.5 Dataset API
Dataset — типизированная абстракция для работы с данными (доступна на Scala и Java), сочетающая преимущества RDD 
(безопасность типов, высокоуровневая трансформация) и DataFrame (оптимизация, ленивое выполнение).  

```scala
// Пример на Scala
case class Person(name: String, age: Int)
val ds = spark.read.json("people.json").as[Person]
ds.filter(_.age > 18).show()
```
                  

