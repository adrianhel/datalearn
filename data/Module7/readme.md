# 7. Apache Spark

### [Назад в Содержание ⤶](/README.md)

<p align="center">
<img src="/data/Module7/img/spark_logo.png" width="50%">
</p>

## 7.1 Знакомство с Apache Spark
> **Apache Spark** — это молниеносный унифицированный аналитический движок, используемый для кластерных вычислений 
> с большими массивами данных (**BigData**, **Hadoop**) с целью параллельного выполнения программ на нескольких узлах.  

### Эко система Apache Spark

<p align="center">
<img src="/data/Module7/img/spark_system.png" width="60%">
</p>

**Spark** представляет собой комбинацию нескольких библиотек: **SQL**, **Dataframes**, **GraphX**, **MLlib** и 
**Spark Streaming**.  

> **Apache Spark** разработан на языке программирования **Scala** и работает на **JVM**.

## 7.2 Установка Apache Spark
[Руководство по установке и настройке Spark (локально)](data/spark_install.md)  

[Руководство по установке и настройке PySpark (в Docker)](data/spark_dc_install.md)

## 7.3 Режимы работы Apache Spark
**Apache Spark** работает в 4 различных режимах:
- **Автономный режим**: все процессы выполняются в рамках одного процесса **JVM**.  
- **Автономный кластерный режим**: используется встроенная в **Spark** система планирования заданий.  
- **Apache Mesos**: рабочие узлы работают на разных компьютерах, но драйвер работает только на главном узле.  
- **Hadoop YARN**: драйверы работают на главном узле приложения и управляются **YARN** в кластере.

## 7.4 Spark Session
**Spark Session** является высокоуровневым интерфейсом, который объединяет функциональность 
**[Spark Context](data/spark_context.md)**, **SQL Context** и **Hive Context** в одном объекте. Он предоставляет доступ 
к функциям **Spark Core**, **Spark SQL**, **Spark Streaming**, **MLlib** и **GraphX**.  

**Spark Session** позволяет разработчикам работать с различными типами данных, включая **RDD**, **DataFrames** 
и **Datasets**, а также выполнять запросы на языке **SQL**, манипулировать структурированными данными и выполнять 
аналитику в реальном времени.  

## 7.5 Драйвер и Исполнители (Driver & Executors)
Любое [Spark приложение](data/spark_worlflow.md) состоит из драйвера (**Driver**) и исполнителей (**Executors**). 
**Driver** – это и есть наша **Spark Session**. Именно в ней задается конфигурация запуска нашего приложения и многое 
другое.  

<p align="center">
<img src="/data/Module7/img/spark_driver.png" width="50%">
</p>

Экзекьюторы нужны для выполнения задач Spark. Одно приложение в Spark – это несколько задач (но может быть и одна). 
Именно в исполнителях (экзекьюторах) происходит обработка данных. Экзекьюторы работают параллельно и потом отдают 
результат драйверу.  

Есть **Worker Node**, которые задаются **Cluster Manager** динамически. Исходя из этих **Worker Node** и доступной памяти 
мы запускаем приложение Spark. На одной **Worker Node** может находиться несколько **Executors**, если будет хватать 
ресурсов. Также в **Executors** может находиться несколько задач.  

## 7.6 Структуры данных в Apache Spark
В Spark есть два основных абстрактных уровня работы с данными в распределенных вычислениях:
- **[RDD](data/rdd.md)** (Resilient Distributed Dataset) — низкоуровневая, гибкая, но менее оптимизированная.  
- **[DataFrame](data/dataframe.md)** & **[Dataset](data/dataset.md)** — высокоуровневые структуры, построенные поверх RDD, оптимизированные для 
производительности, основанные на концепции "Распределенная коллекция с именованными столбцами".  

> Современная разработка в основном ведется с использованием **DataFrame/Dataset API**.  

### Сравнение RDD, DataFrame и Dataset по схеме данных
#### RDD
- Схема данных неявно определяется структурой объектов.  
- Нет встроенной поддержки явного описания схемы.  
- Подходит для работы с неструктурированными или слабо структурированными данными.  

#### DataFrame
- Схема данных явная и включает имена столбцов и типы данных.  
- Поддерживает автоматическое определение схемы при чтении данных из различных источников.  
- Удобен для работы со структурированными данными и предоставляет мощный **API** для обработки данных на уровне **SQL**.   

#### Dataset (Scala/Java)
- Схема данных явная и определяется структурой типизированных объектов (классов).  
- Сочетает в себе преимущества **RDD** (типизированный **API**) и **DataFrame** (явная схема и оптимизация).  
- Подходит для работы с типизированными данными и обеспечивает статическую типизацию и оптимизацию через 
**Catalyst Optimizer**.  

Очевидно, что высокоуровневые структуры имеют ряд [приемуществ](data/spark_advantages_over_rdd.md) над RDD.

## 7.7 Spark Core: базовые функции
В Apache Spark концепции [ленивых вычислений](data/spark_lazy.md) (lazy evaluation), 
[преобразований](data/spark_transformations.md) (transformations) и [действий](data/spark_actions.md) (actions) играют 
ключевую роль в оптимизации и выполнении распределенных вычислений, они составляют жизненный цикл Spark-программы.  

### Жизненный цикл Spark-программы
1. **Создание RDD/DataFrame**: Вы начинаете с исходных данных.
2. **Определение преобразований**: Вы строите цепочку вызовов **map**, **filter**, **join** и т.д. Spark в это время 
строит и оптимизирует внутренний DAG.  
3. **Вызов действия**: Когда вы вызываете **count()**, **collect()** или **save...()**, Spark "просыпается".  
4. **Планирование и выполнение**: Движок Spark:  
   - Делит DAG на этапы (**stages**) на основе широких преобразований.  
   - Создает задачи (**tasks**) для каждого этапа.  
   - Распределяет задачи по исполнителям (**executors**) на кластере.  
5. **Возврат результата**: Результат вычислений возвращается в драйвер-программу или записывается в хранилище.  

> Таким образом, ленивые вычисления — это "двигатель", преобразования — это "маршрут на карте", а действия — 
> это команда "поехали!", которая заставляет двигатель работать и проходить проложенный маршрут.  

## 7.8 Популярные форматы хранения данных
Чтение данных из файлов в Apache Spark является одним из наиболее часто выполняемых действий, так как Spark 
предназначен для работы с большими объемами данных, хранящихся в различных форматах. В Spark вы можете читать данные 
из различных источников, таких как HDFS, S3, локальная файловая система и другие. 

Spark поддерживает множество форматов хранения данных:  
- [Apache Parquet](data/parquet.md)  
- [Apache ORC (Optimized Row Columnar)](data/orc.md)   
- [Apache Avro](data/avro.md)  
- [Delta Lake](data/delta_lake.md)  
- [JSON/CSV](data/json_csv.md)  

> Выбор правильного формата критически важен для производительности, стоимости и удобства работы.  

### Сводная таблица

| *Формат*         | Тип                    | Лучше всего подходит для                | Ключевые особенности                                                |
|------------------|------------------------|-----------------------------------------|---------------------------------------------------------------------|
| ***Parquet***    | Колоночный             | Аналитические запросы, Data Warehousing | Высокая производительность, предикатный pushdown, стандарт де-факто |
| ***Delta Lake*** | Табличный (на Parquet) | Надежные ETL/ELT пайплайны, Lakehouse   | ACID-транзакции, Time Travel, Upserts/Deletes                       |
| ***ORC***        | Колоночный             | Аналитические запросы (часто в Hive)    | Аналог Parquet, хорошая производительность                          |
| ***Avro***       | Роу-ориентированный    | Стриминг, сериализация, сырой слой      | Компактность, быстрая сериализация, эволюция схемы                  |
| ***JSON/CSV***   | Текстовый              | Обмен данными, начальные этапы          | Человекочитаемость, простота                                        |

### Рекомендации
- Используйте **Parquet**. Это безопасный и эффективный выбор для 90% случаев.  
- Нужны транзакции, обновления и надежность, используйте **Delta Lake**. Это естественная эволюция от простого **Parquet**.  
- Для стриминга рассмотрите **Avro**.  
- Избегайте **JSON/CSV** для хранения больших данных, используйте их только как исходный формат для приема данных, который 
затем конвертируется в **Parquet/Delta**.  

## 7.9 Spark SQL: обработка структурированных данных
[Spark SQL](data/spark_sql.md) — компонент Apache Spark, предназначенный для обработки структурированных данных. 
Он предоставляет интерфейс для работы с данными в виде таблиц, поддерживает запросы на языке SQL, а также интеграцию 
с программными интерфейсами на языках Scala, Java, Python и R. Spark SQL оптимизирован для масштабируемой обработки
больших объёмов данных и является основным инструментом для аналитики в рамках экосистемы Spark.  
