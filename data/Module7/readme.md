# 7. Apache Spark

### [Назад в Содержание ⤶](/README.md)

<p align="center">
<img src="/data/Module7/img/spark_logo.png" width="50%">
</p>

## 7.1 Знакомство с Apache Spark
> ***Apache Spark*** — это open-source распределённая вычислительная платформа, предназначенная для обработки 
> больших объемов данных.

**Spark** — это молниеносный унифицированный аналитический движок, используемый для кластерных вычислений с большими 
массивами данных (**BigData**, **Hadoop**) с целью параллельного выполнения программ на нескольких узлах.

### Эко система Apache Spark

<p align="center">
<img src="/data/Module7/img/spark_system.png" width="50%">
</p>

**Spark** представляет собой комбинацию нескольких библиотек: **SQL**, **Dataframes**, **GraphX**, **MLlib** и 
**Spark Streaming**.  

> **Apache Spark** разработан на языке программирования **Scala** и работает на **JVM**.

## 7.2 Установка Apache Spark

[Руководство по установке и настройке Spark (локально)](data/spark_install.md)  

[Руководство по установке и настройке PySpark (в Docker)](data/spark_dc_install.md)

## 7.3 Режимы работы Apache Spark
**Apache Spark** работает в 4 различных режимах:
- **Автономный режим**: все процессы выполняются в рамках одного процесса **JVM**.  
- **Автономный кластерный режим**: используется встроенная в **Spark** система планирования заданий.  
- **Apache Mesos**: рабочие узлы работают на разных компьютерах, но драйвер работает только на главном узле.  
- **Hadoop YARN**: драйверы работают на главном узле приложения и управляются **YARN** в кластере.

## 7.4 Spark Session
**Spark Session** является высокоуровневым интерфейсом, который объединяет функциональность 
**[Spark Context](data/spark_context.md)**, **SQL Context** и **Hive Context** в одном объекте. Он предоставляет доступ 
к функциям **Spark Core**, **Spark SQL**, **Spark Streaming**, **MLlib** и **GraphX**.  

**Spark Session** позволяет разработчикам работать с различными типами данных, включая **RDD**, **DataFrames** 
и **Datasets**, а также выполнять запросы на языке **SQL**, манипулировать структурированными данными и выполнять 
аналитику в реальном времени.  

## 7.5 Драйвер и Исполнители (Driver & Executors)
Любое Spark приложение состоит из драйвера (Driver) и исполнителей (Executors). Driver - это и есть наша Spark Session. 
Именно в ней, как мы уже поняли, задается конфигурация запуска нашего приложения и многое другое.   

<p align="center">
<img src="/data/Module7/img/spark_driver.png" width="40%">
</p>

Экзекьюторы нужны для выполнения задач Spark. Одно приложение в Spark – это несколько задач (но может быть и одна). 
Именно в исполнителях (экзекьюторах) происходит обработка данных. Экзекьюторы работают параллельно и потом отдают 
результат драйверу.  

Есть Worker Node, которые задаются Cluster Manager динамически. Исходя из этих Worker Node и доступной памяти 
мы запускаем приложение Spark. На одной Worker Node может находиться несколько Executors, если будет хватать ресурсов.
Также в Executors может находиться несколько задач.  

## 7.6 