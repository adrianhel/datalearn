# 7. Apache Spark

### [Назад в Содержание ⤶](/README.md)

<p align="center">
<img src="/data/Module7/img/spark_logo.png" width="50%">
</p>

## 7.1 Знакомство с Apache Spark
> **Apache Spark** — это open-source распределённая вычислительная платформа, предназначенная для обработки 
> больших объемов данных.

**Spark** — это молниеносный унифицированный аналитический движок, используемый для кластерных вычислений с большими 
массивами данных (**BigData**, **Hadoop**) с целью параллельного выполнения программ на нескольких узлах.

### Эко система Apache Spark

<p align="center">
<img src="/data/Module7/img/spark_system.png" width="50%">
</p>

**Spark** представляет собой комбинацию нескольких библиотек: **SQL**, **Dataframes**, **GraphX**, **MLlib** и 
**Spark Streaming**.  

> **Apache Spark** разработан на языке программирования **Scala** и работает на **JVM**.

## 7.2 Установка Apache Spark

[Руководство по установке и настройке Spark (локально)](data/spark_install.md)  

[Руководство по установке и настройке PySpark (в Docker)](data/spark_dc_install.md)

## 7.3 Режимы работы Apache Spark
**Apache Spark** работает в 4 различных режимах:
- **Автономный режим**: все процессы выполняются в рамках одного процесса **JVM**.  
- **Автономный кластерный режим**: используется встроенная в **Spark** система планирования заданий.  
- **Apache Mesos**: рабочие узлы работают на разных компьютерах, но драйвер работает только на главном узле.  
- **Hadoop YARN**: драйверы работают на главном узле приложения и управляются **YARN** в кластере.

## 7.4 Spark Session
**Spark Session** является высокоуровневым интерфейсом, который объединяет функциональность 
**[Spark Context](data/spark_context.md)**, **SQL Context** и **Hive Context** в одном объекте. Он предоставляет доступ 
к функциям **Spark Core**, **Spark SQL**, **Spark Streaming**, **MLlib** и **GraphX**.  

**Spark Session** позволяет разработчикам работать с различными типами данных, включая **RDD**, **DataFrames** 
и **Datasets**, а также выполнять запросы на языке **SQL**, манипулировать структурированными данными и выполнять 
аналитику в реальном времени.  

## 7.5 Драйвер и Исполнители (Driver & Executors)
Любое [Spark приложение](data/spark_worlflow.md) состоит из драйвера (Driver) и исполнителей (Executors). Driver – это 
и есть наша Spark Session. Именно в ней задается конфигурация запуска нашего приложения и многое другое.  

<p align="center">
<img src="/data/Module7/img/spark_driver.png" width="40%">
</p>

Экзекьюторы нужны для выполнения задач Spark. Одно приложение в Spark – это несколько задач (но может быть и одна). 
Именно в исполнителях (экзекьюторах) происходит обработка данных. Экзекьюторы работают параллельно и потом отдают 
результат драйверу.  

Есть Worker Node, которые задаются Cluster Manager динамически. Исходя из этих Worker Node и доступной памяти 
мы запускаем приложение Spark. На одной Worker Node может находиться несколько Executors, если будет хватать ресурсов.
Также в Executors может находиться несколько задач.  

## 7.6 Структуры данных в Apache Spark
В Spark есть два основных типа структурированных данных:
- **[RDD](data/rdd.md)** (Resilient Distributed Dataset) — низкоуровневая, гибкая, но менее оптимизированная.  
- **[DataFrame](data/dataframe.md)** & **[Dataset](data/dataset.md)** — высокоуровневые, оптимизированные для 
производительности, основанные на концепции "Распределенная коллекция с именованными столбцами".  

> Современная разработка в основном ведется с использованием DataFrame/Dataset API.  

### Сравнение RDD, DataFrame и Dataset по схеме данных
##### RDD
- Схема данных неявно определяется структурой объектов.  
- Нет встроенной поддержки явного описания схемы.  
- Подходит для работы с неструктурированными или слабо структурированными данными.  

##### DataFrame
- Схема данных явная и включает имена столбцов и типы данных.  
- Поддерживает автоматическое определение схемы при чтении данных из различных источников.  
- Удобен для работы со структурированными данными и предоставляет мощный API для обработки данных на уровне SQL.   

##### Dataset (Scala/Java)
- Схема данных явная и определяется структурой типизированных объектов (классов).  
- Сочетает в себе преимущества RDD (типизированный API) и DataFrame (явная схема и оптимизация).  
- Подходит для работы с типизированными данными и обеспечивает статическую типизацию и оптимизацию через 
**Catalyst Optimizer**.  

## 7.7 Фундаментальные концепции в Apache Spark
В Apache Spark концепции ленивых вычислений ([lazy evaluation](data/spark_lazy.md)), преобразований
([transformations](data/spark_transformations.md) и действий ([actions](data/spark_actions.md) играют ключевую роль 
в оптимизации и выполнении распределенных вычислений.  

## 7.8 Популярные форматы хранения данных
Spark поддерживает множество форматов:  
- [Apache Parquet](data/parquet.md)  
- [Apache ORC (Optimized Row Columnar)](data/orc.md)   
- [Avro](data/avro.md)  
- [Delta Lake](data/delta_lake.md)  
- [JSON/CSV](data/json_csv.md)  

> Выбор правильного формата критически важен для производительности, стоимости и удобства работы.  

### Сводная таблица

| Формат           | Тип                    | Лучше всего подходит для                | Ключевые особенности                                                |
|------------------|------------------------|-----------------------------------------|---------------------------------------------------------------------|
| ***Parquet***    | Колоночный             | Аналитические запросы, Data Warehousing | Высокая производительность, предикатный pushdown, стандарт де-факто |
| ***Delta Lake*** | Табличный (на Parquet) | Надежные ETL/ELT пайплайны, Lakehouse   | ACID-транзакции, Time Travel, Upserts/Deletes                       |
| ***ORC***        | Колоночный             | Аналитические запросы (часто в Hive)    | Аналог Parquet, хорошая производительность                          |
| ***Avro***       | Роу-ориентированный    | Стриминг, сериализация, сырой слой      | Компактность, быстрая сериализация, эволюция схемы                  |
| ***JSON/CSV***   | Текстовый              | Обмен данными, начальные этапы          | Человекочитаемость, простота                                        |

### Рекомендации
- Используйте Parquet. Это безопасный и эффективный выбор для 90% случаев.  
- Нужны транзакции, обновления и надежность, используйте Delta Lake. Это естественная эволюция от простого Parquet.  
- Для стриминга рассмотрите Avro.  
- Избегайте JSON/CSV для хранения больших данных, используйте их только как исходный формат для приема данных, который 
затем конвертируется в Parquet/Delta.  