# 7. Apache Spark

### [Назад в Содержание ⤶](/README.md)

<p align="center">
<img src="/data/Module7/img/spark_logo.png" width="50%">
</p>

## 7.1 Знакомство с Apache Spark
> **Apache Spark** — это молниеносный унифицированный аналитический движок, используемый для кластерных вычислений 
> с большими массивами данных (**BigData**, **Hadoop**) с целью параллельного выполнения программ на нескольких узлах.  

### Эко система Apache Spark

<p align="center">
<img src="/data/Module7/img/spark_system.png" width="60%">
</p>

**Spark** представляет собой комбинацию нескольких библиотек: **SQL**, **Dataframes**, **GraphX**, **MLlib** и 
**Spark Streaming**.  

> **Apache Spark** разработан на языке программирования **Scala** и работает на **JVM**.

## 7.2 Установка Apache Spark
[Руководство по установке и настройке Spark (локально)](data/spark_install.md)  

[Руководство по установке и настройке PySpark (в Docker)](data/spark_dc_install.md)

## 7.3 Режимы работы Apache Spark
**Apache Spark** работает в 4 различных режимах:  
   - **Автономный режим**: все процессы выполняются в рамках одного процесса **JVM**.  
   - **Автономный кластерный режим**: используется встроенная в **Spark** система планирования заданий.  
   - **Apache Mesos**: рабочие узлы работают на разных компьютерах, но драйвер работает только на главном узле.  
   - **Hadoop YARN**: драйверы работают на главном узле приложения и управляются **YARN** в кластере.  

## 7.4 Spark Session
**Spark Session** является высокоуровневым интерфейсом, который объединяет функциональность 
**[Spark Context](data/spark_context.md)**, **SQL Context** и **Hive Context** в одном объекте. Он предоставляет доступ 
к функциям **Spark Core**, **Spark SQL**, **Spark Streaming**, **MLlib** и **GraphX**.  

**Spark Session** позволяет разработчикам работать с различными типами данных, включая **RDD**, **DataFrames** 
и **Datasets**, а также выполнять запросы на языке **SQL**, манипулировать структурированными данными и выполнять 
аналитику в реальном времени.  

## 7.5 Драйвер и Исполнители (Driver & Executors)
Любое [Spark приложение](data/spark_worlflow.md) состоит из драйвера (**Driver**) и исполнителей (**Executors**). 
**Driver** – это и есть наша **Spark Session**. Именно в ней задается конфигурация запуска нашего приложения и многое 
другое.  

<p align="center">
<img src="/data/Module7/img/spark_driver.png" width="50%">
</p>

Экзекьюторы нужны для выполнения задач Spark. Одно приложение в Spark – это несколько задач (но может быть и одна). 
Именно в исполнителях (экзекьюторах) происходит обработка данных. Экзекьюторы работают параллельно и потом отдают 
результат драйверу.  

Есть **Worker Node**, которые задаются **Cluster Manager** динамически. Исходя из этих **Worker Node** и доступной памяти 
мы запускаем приложение Spark. На одной **Worker Node** может находиться несколько **Executors**, если будет хватать 
ресурсов. Также в **Executors** может находиться несколько задач.  

## 7.6 Структуры данных в Apache Spark
В Spark есть два основных абстрактных уровня работы с данными в распределенных вычислениях:
   - **[RDD](data/rdd.md)** (Resilient Distributed Dataset) — низкоуровневая, гибкая, но менее оптимизированная.  
   - **[DataFrame](data/dataframe.md)** & **[Dataset](data/dataset.md)** — высокоуровневые структуры, построенные поверх RDD, оптимизированные для 
   производительности, основанные на концепции "Распределенная коллекция с именованными столбцами".  

> Современная разработка в основном ведется с использованием **DataFrame/Dataset API**.  

### Сравнение RDD, DataFrame и Dataset по схеме данных
#### RDD
   - Схема данных неявно определяется структурой объектов.  
   - Нет встроенной поддержки явного описания схемы.  
   - Подходит для работы с неструктурированными или слабо структурированными данными.  

#### DataFrame
   - Схема данных явная и включает имена столбцов и типы данных.  
   - Поддерживает автоматическое определение схемы при чтении данных из различных источников.  
   - Удобен для работы со структурированными данными и предоставляет мощный **API** для обработки данных на уровне **SQL**.   

#### Dataset (Scala/Java)
   - Схема данных явная и определяется структурой типизированных объектов (классов).  
   - Сочетает в себе преимущества **RDD** (типизированный **API**) и **DataFrame** (явная схема и оптимизация).  
   - Подходит для работы с типизированными данными и обеспечивает статическую типизацию и оптимизацию через 
   **Catalyst Optimizer**.  

Очевидно, что высокоуровневые структуры имеют ряд [приемуществ](data/spark_advantages_over_rdd.md) над RDD.

## 7.7 Spark Core
В Apache Spark концепции [ленивых вычислений](data/spark_lazy.md) (lazy evaluation), 
[преобразований](data/spark_transformations.md) (transformations) и [действий](data/spark_actions.md) (actions) играют 
ключевую роль в оптимизации и выполнении распределенных вычислений, они составляют жизненный цикл Spark-программы.  

### Жизненный цикл Spark-программы
1. **Создание RDD/DataFrame**: Вы начинаете с исходных данных.
2. **Определение преобразований**: Вы строите цепочку вызовов **map**, **filter**, **join** и т.д. Spark в это время 
строит и оптимизирует внутренний DAG.  
3. **Вызов действия**: Когда вы вызываете **count()**, **collect()** или **save...()**, Spark "просыпается".  
4. **Планирование и выполнение**: Движок Spark:  
   - Делит DAG на этапы (**stages**) на основе широких преобразований.  
   - Создает задачи (**tasks**) для каждого этапа.  
   - Распределяет задачи по исполнителям (**executors**) на кластере.  
5. **Возврат результата**: Результат вычислений возвращается в драйвер-программу или записывается в хранилище.  

> Таким образом, ленивые вычисления — это "двигатель", преобразования — это "маршрут на карте", а действия — 
> это команда "поехали!", которая заставляет двигатель работать и проходить проложенный маршрут.  

## 7.8 Популярные форматы хранения данных
Чтение данных из файлов в Apache Spark является одним из наиболее часто выполняемых действий, так как Spark 
предназначен для работы с большими объемами данных, хранящихся в различных форматах. В Spark вы можете читать данные 
из различных источников, таких как HDFS, S3, локальная файловая система и другие. 

Spark поддерживает множество форматов хранения данных:  
   - [Apache Parquet](data/parquet.md)   
   - [Apache ORC (Optimized Row Columnar)](data/orc.md)   
   - [Apache Avro](data/avro.md)  
   - [Delta Lake](data/delta_lake.md)  
   - [JSON/CSV](data/json_csv.md)  

> Выбор правильного формата критически важен для производительности, стоимости и удобства работы.  

### Сводная таблица

| *Формат*         | Тип                    | Лучше всего подходит для                | Ключевые особенности                                                |
|------------------|------------------------|-----------------------------------------|---------------------------------------------------------------------|
| ***Parquet***    | Колоночный             | Аналитические запросы, Data Warehousing | Высокая производительность, предикатный pushdown, стандарт де-факто |
| ***Delta Lake*** | Табличный (на Parquet) | Надежные ETL/ELT пайплайны, Lakehouse   | ACID-транзакции, Time Travel, Upserts/Deletes                       |
| ***ORC***        | Колоночный             | Аналитические запросы (часто в Hive)    | Аналог Parquet, хорошая производительность                          |
| ***Avro***       | Роу-ориентированный    | Стриминг, сериализация, сырой слой      | Компактность, быстрая сериализация, эволюция схемы                  |
| ***JSON/CSV***   | Текстовый              | Обмен данными, начальные этапы          | Человекочитаемость, простота                                        |

### Рекомендации
- Используйте **Parquet**. Это безопасный и эффективный выбор для 90% случаев.  
- Нужны транзакции, обновления и надежность, используйте **Delta Lake**. Это естественная эволюция от простого **Parquet**.  
- Для стриминга рассмотрите **Avro**.  
- Избегайте **JSON/CSV** для хранения больших данных, используйте их только как исходный формат для приема данных, который 
затем конвертируется в **Parquet/Delta**.  

## 7.9 Spark SQL
[Spark SQL](data/spark_sql.md) — компонент Apache Spark, предназначенный для обработки структурированных данных. 
Он предоставляет интерфейс для работы с данными в виде таблиц, поддерживает запросы на языке SQL, а также интеграцию 
с программными интерфейсами на языках Scala, Java, Python и R. Spark SQL оптимизирован для масштабируемой обработки
больших объёмов данных и является основным инструментом для аналитики в рамках экосистемы Spark.  

## 7.10 Spark Streaming
[Spark Streaming](data/spark_streaming.md) — компонент Apache Spark для масштабируемой, отказоустойчивой и эффективной 
потоковой обработки данных в реальном времени. Он расширяет архитектуру Spark, позволяя анализировать непрерывные потоки 
входящих данных, обрабатывая их в виде небольших пакетов (batches) с низкой задержкой.  

Механизмы работы с окнами, состояния и интеграция с внешними источниками делают Spark Streaming универсальным 
инструментом для решения задач анализа потоковых данных в рамках Big Data.  

## 7.11 Spark MLlib
[Spark MLlib](data/spark_ml.md) — масштабируемая библиотека машинного обучения, являющаяся составной частью Apache Spark.  
MLlib предоставляет высокоуровневые API для создания, обучения и применения моделей машинного обучения на больших 
объемах данных, поддерживает распределённые вычисления, что позволяет обрабатывать терабайты информации 
с использованием кластеров.  

   - Поддержка основных алгоритмов машинного обучения  
   - Поддержка различных форматов данных (RDD, DataFrame, Dataset)  
   - Встроенные средства для предобработки данных и построения конвейеров (pipelines)  
   - Интеграция с экосистемой Spark и другими инструментами Big Data  

## 7.12 Spark GraphX
[Spark GraphX](data/spark_graphx.md) — библиотека Apache Spark, предназначенная для параллельной обработки и анализа 
графов.  
GraphX объединяет парадигму обработки данных RDD (Resilient Distributed Dataset) с возможностями представления и 
трансформации графовых структур, что позволяет эффективно работать с большими графами в распределённой среде.  

   - GraphX позволяет моделировать и анализировать сложные взаимосвязи между объектами в больших данных.  
   - Использование распределённых вычислений обеспечивает масштабируемость анализа графов.  
   - Гибкая интеграция с другими компонентами Spark облегчает построение комплексных аналитических пайплайнов.  

## 7.13 Преимущества Spark для аналитики
Практические применения [Spark в аналитике](data/spark_da.md):  
   - Обработка и агрегация больших объёмов логов и событий (например, анализ поведения пользователей на веб-сайтах).  
   - Построение отчётов и дашбордов в реальном времени для мониторинга бизнес-процессов.  
   - Машинное обучение: построение моделей прогнозирования, кластеризация, рекомендационные системы.  
   - Анализ графовых данных: выявление сообществ, поиск кратчайших путей, анализ связности.  
   - Интеграция и очистка данных из различных источников для построения витрин данных (Data Marts).  
   - Финансовая аналитика: анализ транзакций, обнаружение мошенничества.  

## 7.14 Ограничения Spark
Несмотря на высокую производительность и широкие возможности, Spark обладает [рядом ограничений](data/spark_limits.md) 
и особенностей, критичных для проектирования и эксплуатации систем обработки данных.  